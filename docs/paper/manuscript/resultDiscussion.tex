\section{Results and Discussion}
\label{sec:results-discussion}


%\section{Study Settings} \label{study-settings}

In order to guide the discussion of the results, we apply the Goal Question Metric (GQM) method~\cite{gqm}, so that it helps structuring the context, the object of study, its properties, the goal, and how this latter can be operationalized and answered. In this section we first discuss the goals, questions and metrics of our investigation (Subsection~\ref{sec:gqm}), then evaluate our empirical study  accordingly (Subsection~\ref{sec:assessment}), and finally discuss its limitations (Section~\ref{sec:threats}).

\subsection{Goals, Questions, and Metrics} \label{sec:gqm}

Our goal is to assess the configurability and flexibility of \hpl{} (Sections~\ref{sec:domainDesign} and~\ref{sec:implementation}), regarding the application engineering phase for software product development and within the context of the different artifacts.  Since \hpl{} emerged from a systematic evolution of \hp{}, this later is also assessed as a baseline. According to GQM, Table~\ref{tab:gqm-goal} summarizes the general evaluation goal of our work.

\begin{table}[h]
\begin{center}
\begin{tabular}{||l||l||}
  \hline
  \textbf{Purpose} & assess   \\  \hline
  \textbf{Issue} & the configurability and flexibility of\\  \hline
  \textbf{Object} & \hpl{} and \hp  \\ \hline
  \textbf{Viewpoint} & application engineering perspective  \\ \hline
  \textbf{Context} & different artifacts  \\ \hline
\end{tabular}
\caption{GQM goal of this research.}
\label{tab:gqm-goal}
\end{center}
\end{table}

In Section~\ref{sec:hephaestus}, we characterized the \emph{context}, the \emph{viewpoint}, and part of the \emph{object} of the initial Hephaestus design, which was targeted
to manage variability in use case scenarios, and its evolution to support variability in other assets.
In Sections~\ref{sec:domainDesign} and~\ref{sec:implementation}, we detailed part of the \emph{object}, i.e., \hpl.
This section proceeds with the analysis of \hpl{} and \hp{}, using a qualitative and quantitative evaluation that answers our GQM metrics, thus meeting the \emph{purpose} and \emph{issue} components of the study's goal.

From the study's goal presented in Table~\ref{tab:gqm-goal}, we derive questions that best characterize our study. Moreover,
related to each question, we use one or more quantitative or qualitative metrics to indicate the compliance level of the techniques in relation to
the study goal. Qualitative assessment ``metrics" are also conceived in the GQM method~\cite{gqm}.
In what follows, we present the questions (\textbf{Q}) and the metrics (\textbf{M}) of our GQM model and explain how they trace to the study's goal.

\begin{enumerate}[Q1]
\item \emph{Is the variability mechanism sufficiently expressive?}
\begin{itemize}
\item Metric \textbf{M1.1}: Is there support for open data types?
\item Metric \textbf{M1.2}: Is there support for open functions?
\item Metric \textbf{M1.3}: Is there support for the instantiation of one SPL asset?
\item Metric \textbf{M1.4}: Is there support for the composition of assets without having to instantiate all assets (over features)?
\end{itemize}

\item \emph{Given a fixed number of assets, what is the effort to address a new configuration?}
\begin{itemize}
\item Metric \textbf{M2.1}: Number of modules changed.
\item Metric \textbf{M2.2}: Number of lines of code changed.
\item Metric \textbf{M2.3}: Number of modules created.
\item Metric \textbf{M2.4}: Number of lines of code created.
\item Metric \textbf{M2.5}: Is there automation support?
\item Metric \textbf{M2.6}: What is the analytical complexity of the effort?
\end{itemize}


\item \emph{What is the effort to address a new asset?}
\begin{itemize}
\item Metric \textbf{M3.1}: Number of modules changed.
\item Metric \textbf{M3.2}: Number of code artifacts changed.
\item Metric \textbf{M3.3}: Number of modules created.
\item Metric \textbf{M3.4}: Number of code artifacts created.
\item Metric \textbf{M3.5}: Is there automation support?
\end{itemize}

%@all: should we use a metric like degree of scattering below? also degree of tangling?
\item \emph{Is modular management of asset related variability supported?}
\begin{itemize}
% \item Metric \textbf{M4} (Yes/No): The technique provides support for feature modularization.
\item Metric \textbf{M4.1}: Are the changes related to variability of SPL asset homogeneous or heterogeneous?
\item Metric \textbf{M4.2}: Are the changes related to variability of SPL asset localized or scattered?
\item Metric \textbf{M4.3}: What is the possibility of automation of the changes related to variability of SPL asset?
%\item Metric \textbf{M4.4}: What is the value for the Concern Diffusion over Components (CDC) metric?
%\item Metric \textbf{M4.5}: What is the value for the Concern Diffusion over Operations (CDO) metric?
\end{itemize}
\end{enumerate}


%@Lucineia: please ensure that in  Section 3.1 (Domain Analysis) we make this classification or give elements so that we can say we address such things in the paragraph below.
%           In Section 3.1, after the final bullet list, please add this discussion.  We only slight mention this in Seciton 2.1.
Questions Q1-Q4 represent relevant characteristics of the development of \hpl{} when compared to \hp. Q1 traces to the configurability issue, addressing the required expressiveness of the underlying variability management mechanism. Correspondingly, Metrics M1.1, M1.2, M1.3 and M1.4 investigate whether there is support for the types of variability in Hephaestus-PL, classified as open data types, open functions, single asset instantiation, and assets composition, as explained in Section~\ref{sec:domainAnalysis}.
In particular, metric M1.4 focuses on a current issue of Hephaestus: the need to instantiate more than one SPL asset defined in \texttt{SPLModel} and \texttt{InstanceModel} algebraic types.
Likewise, Q2 also deals with the configurability issue, but addresses it within a predefined scope of assets. Its metrics then address the effort to add a new configuration, from different granularity perspectives (modules and lines of code), automation support, and analytical complexity, the latter being relevant for scalability.
Differently, Q3 traces to the flexibility issue, by considering the necessary evolution effort to address variability in a new asset. This question is refined by metrics from different granularity perspectives (code artifacts represent data types and functions, except modules) and also considering automation support. Finally, Q4 also refers to the flexibility issue and assesses whether modularity in handling asset related variability is supported or not, an important property to the reactive approach for SPL development since it potentially supports the introduction of new features. 
The homogeneity (metric 4.1) refers to the kinds of Haskell syntactic structures (data types, functions, classes) associated with the changes. We consider homogeneous when there is only one type of syntactic structure to change and heterogeneous otherwise. 
The location of the changes (metric 4.2) is assessed on the modules with tangled code of features and can be defined as localized when it involves only one module or scattered otherwise. 
At the end, the metric M4.3 is defined as a result of metric M4.1 and M4.2. Thus, M4.3 can be high when we observe the values homogeneous to M4.1 and located to M4.2 otherwise we define M4.3 as low.



\subsection{Assessment} \label{sec:assessment}

Given the GQM setup from Subsection~\ref{sec:gqm}, we now proceed to evaluating the design and implementation techniques for managing variability (Sections~\ref{sec:domainDesign} and~\ref{sec:implementation}). Table~\ref{tab:assessment-hpl-hp} summarizes the assessment to \hp{} and \hpl{} of the metrics associated to questions defined in the our GQM model.


\begin{table}[h]
\begin{center}
\begin{tabular}{||c|| p{5cm} || p{5cm}||}
  \hline
  \textbf{Metric} & \textbf{Hephaestus} & \textbf{Hephaestus-PL}   \\  \hline  \hline 
  \textbf{M1.1} & No  & Yes \\  \hline
  \textbf{M1.2} & No  & Yes \\  \hline
  \textbf{M1.3} & Yes & Yes \\  \hline
  \textbf{M1.4} & No  & Yes \\  \hline \hline   
  \textbf{M2.1} & 1  & 0 \\  \hline
  \textbf{M2.2} & $9n$  & 0 \\  \hline
  \textbf{M2.3} & 0  & 0 \\  \hline
  \textbf{M2.4} & $8n$  & 0 \\  \hline  
  \textbf{M2.5} & No & Yes \\  \hline  
  \textbf{M2.6} & $O(n)$ & $O(k)$ \\  \hline  \hline 
  \textbf{M3.1} & 5  & 1 \\  \hline
  \textbf{M3.2} & 6  & 4 \\  \hline
  \textbf{M3.3} & 4 & 4 \\  \hline
  \textbf{M3.4} & low, depends of new asset &  low, depends of new asset \\  \hline  
  \textbf{M3.5} & No & No \\  \hline \hline 
  \textbf{M4.1} & heterogeneous  & homogeneous \\  \hline
  \textbf{M4.2} & scattered      & localized \\  \hline
  \textbf{M4.3} & low            & high \\  \hline
%  \textbf{M4.4} & ?              & ? \\  \hline
%  \textbf{M4.5} & ?              & ? \\  \hline  
%  \textbf{M4}   & No (heterogeneous and scattered changes) & Yes (homogeneous and localized changes)  \\  \hline \hline        
\end{tabular}
\caption{Summary of the assessment of the metrics of GQM model}
\label{tab:assessment-hpl-hp}
\end{center}
\end{table}


Regarding Q1, when evaluated in \hpl{}, there is sufficient expressiveness to support all variability types in Hephaestus.
The support for variability in data types and functions (metrics M1.1 and M1.2) is guaranteed by the transformational approach of variability management used in \hpl{}, i.e., using metaprogramming operations 
%with the support of meta data structure containing SPL asset informations we 
to extend the data types and functions.
The support for the composition of assets, i.e., instantiation of a product with one or more assets (metrics M1.3 and M1.4) is guaranteed by the \texttt{build} process of the \hpl's kernel and metaprogramming operations generating a \hpl{} variant that represents an instance of product which combines any assets of the \hpl's Feature Model.
When evaluated in \hp{}, that is a variant manually generated that supports derivation of products using a compositional approach of artifacts and transformations into CK to generate a new product, we observe there is not support for variability management in data types and functions by the introduction of new assets and for the composition of any assets. 
%In the first \hp{} variant the variability management of use case scenarios used MSVCM (Modeling Scenario Variability as Crosscutting Mechanisms) such as aspects' concept. 
Related to M1.4, the generation of \hp{} variant that supports a composition of assets without having to instantiate all assets requires noticiable effort because it is necessary to change several code artifacts (modules, data types and functions). 
This \textit{ad hoc} change in different parts of the code is an error-prone activity and it is not usually done. Alternatively, we could use the concept of \textit{Over Feature} to generate new \hp{} variants because the impact is considered small and limited into \texttt{main} function.

%The support for variability in data types, functions and the composition of assets is guaranteed by operations in the meta-programming module in the process of generating a variant that represents an instance of product which combines any assets of the SPL Feature Model.....

Regarding Q2, when evaluated in \hpl{}, the effort to address a new configuration from a fixed number of assets is constant and small and represents the effort to specify the new product configuration for \hpl{} with the selected assets, whereas the assets of the desired configuration are already integrated into \hpl. Therefore, a measure for the metric M2.6 is constant and close to zero ($\simeq O(k)$). Likewise, the measure for the metrics M2.1, M2.2, M2.3 and M2.4 is zero, i.e., there are not created and modified modules. Regarding the metric M2.5, there is an automation support in \hpl{} to generate new asset configurations based on the \texttt{build} process, \hpl's transformations and operations of metaprogramming which treat the variability in \hpl.

When evaluated in \hp{}, the effort to address a new configuration of assets can be seen in two ways: by using or not the concept of \textit{Over Feature}. 
First, using the concept of \textit{Over Feature} and a \hp{} product that supports a set of assets where the desired configuration is a subset of these assets, the effort is considered small with impact only on the \texttt{Main.hs} module that needs some changes in
the code. Therefore, the measure of the metric 2.1 is one module and the measure of the metric 2.3 is zero module (i.e., no module is created). Lines of code related to selected assets are changed and created in the module \texttt{Main.hs}, i.e., importing the modules of algebraic data types, parser and output format, reading the SPL asset file in the input \hp{} properties file, executing the asset parser, updating the \texttt{createSPL} function after execute the asset parser
%, inserting the definition of empty asset into empty product function for the build process 
and executing the call of the export function of the asset in the output format. This represents about nine changed lines (metric M2.2) and eight created lines (metric M2.4) in \texttt{Main.hs} module. 
%In \texttt{ConfigurationKnowledge/Interpreter.hs} module are introduced about four new lines to the definition of empty asset into empty product function for the \texttt{build} process. 
With respect to the metric M2.5 there is not automation support in \hp{} and analytical complexity of the effort (metric M2.6) using the concept of \textit{Over Feature} in \hp{} is the sum of the number of changed and created lines (metric M2.2 and M2.4) multiplied by the number of assets ($n$) of the desired configuration, i.e., $(M2.2 + M2.4) * n \simeq O(n)$. Therefore, the effort is proportionally linear to the number of assets of the configuration.

Analyzing now the effort to address a new configuration of assets when we did not use \textit{Over Feature} in \hp{}, we observe that the impact is quite high because it is necessary to introduce into the \hp{} variant the code related to selected assets and this requires a good understanding the code and changes in several modules, data types and functions. Moreover, being a manual activity is more error-prone and to check invalid settings is not an easy task to be done manually. Thus, only to use code of the assets of the selected product configuration would be necessary to copy the source code of \hp{} variant to a new directory and remove the non-selected features. Currently \hp{} does not work like this.

Regarding Q3 refers to reactive process, both \hpl{} and \hp{} tools, the effort to address a new asset considers the major and initial effort to generate the elements of the new asset, such as the abstract data types, the transformations, the parser function and the output format function. This effort is the same in both tools and assessing it is outside the scope of this paper. After generation of asset's elements, there is the effort to integrate these elements of new asset into \hpl{} and \hp{}. We evaluate this later effort.
When evaluated in \hpl{}, the number of changed modules and code artifacts (metrics M3.1 and M3.2) is only one changed module (\texttt{MetaData.hs}) and four changed code artifacts. In \texttt{MetaData.hs} module we need to change the \texttt{featuremodel} and \texttt{configurationKnowledge} functions that build the \hpl's FM and CK and to change the \texttt{assetMetaData} and \texttt{exportMetaData} lists that contain the meta data structures that support metaprogramming operations. The number of created modules and code artifacts (metrics M3.3 and M3.4) are four created modules (\texttt{Types.hs} contains the algebraic data types, \texttt{<NewAsset>.hs} contains the transformations that manage asset's variabilities, \texttt{<NewAssetFomartParser>.hs} contains the asset parser function and \texttt{<NewAssetOutputFormat>.hs} contains a asset output format function) where the sum of created code artifacts (data types and functions) varies depending on the asset. 
%Besides, we have up to fifty created lines of code in the \texttt{MetaData.hs} module.
Currently, there is not automation support in \hpl{} to address a new asset (metric M3.5) but it is possible in the future using a minimum set of variables with the automatic or semi-automatic derivation of the asset meta data informations into asset elements since the meta data estructures have a regular structure and the implementation of asset follow some design rules to enable this automatic or semi-automatic derivation. 
% in the future they may suffer simplifications to automate and derive some informations to execution of the metaprogramming operations.
When evaluated in \hp{}, the number of changed modules and code artifacts (metrics M3.1 and M3.2) are five changed modules (\texttt{ConfigurationKnowledge/Types.lhs}, \\ \texttt{ConfigurationKnowledge/Interpreter.hs}, \\ \texttt{Parsers/XML/XmlConfigurationKnowledge.hs}, \\ \texttt{ExportProduct.hs} and \texttt{Main.hs}) and six changed code artifacts that are \texttt{SPLModel} and \texttt{InstanceModel} data types and \texttt{build}, \texttt{xml2Transformation}, \texttt{exportProduct} and \texttt{main} functions.
The number of created modules and code artifacts (metrics M3.3 and M3.4) are also four created modules (\texttt{Transformations/<newAsset>.hs}, \texttt{<newAsset>/Types.hs}, \\ \texttt{<newAsset>/Parsers/<newAssetParser>.hs} and \\  \texttt{<newAsset>/PrettyPrinter/<newAssetOutputFormat>.hs}) and the number of created code artifacts to represent the elements of the new asset (abstract data types, transformations, parser function and output format function) varies depending on the asset.
In both \hpl{} and \hp{} the number of changed and created code artifacts is small because Haskell is a declarative (functional) language.
In \hp{} there is not automation support to address a new asset (metric M3.5) and it is more difficult than \hpl{} to implement some automation because in \hp{} asset code is scattered in five modules and its format is heterogeneous. 
%As consequence, it becomes more difficult to understand, maintenance and reuse of code besides the possibility of evaluating a automation support.

In terms of feature modularization (Q4), we observe that both \hpl{} and \hp{} support modularity in the elements of SPL asset which are defined in independent modules and have their implementation entirely confined to specific modules, like the four modules that define respectively the algebraic data types, transformations, parser and output format of the asset.
However, there is code associated with SPL asset that is scattered and tangled with code from other SPL asset in some modules. Therefore, to assess if the management of SPL asset related variability is modular we define three metrics.
We evaluate the homogeneity (metric M4.1), the location (metric M4.2) and the possibility of automation (metric M4.3) of the changes related to variation points of SPL asset in the \hpl{} and \hp{} objects as qualitative criteria for the Q4 question. 
%In addition, we use two metrics related to concerns (SPL assets) that are already known, Concern Diffusion over Components (CDC) and Concern Diffusion over Operations (CDO). CDC was applied to the modules and CDO was applied to the functions related concerns.
In \hp{} we define the changes to address an SPL asset as heterogeneous, scattered and therefore with lower possibility of automation. They are heterogeneous because they refer to different types of changes such as insertion of fields in data types, defining new class instances, defining new functions and introducing new sentences in functions. The changes are scattered because they occur in five different \hp{} modules (as presented in \hp{}'s metric M3.1) and 
%\hp{} does not have modular support for the variability management of SPL asset. 
the \hp{} modules that address variability of SPL asset need to be updated manually and it is a error-prone activity (i.e., changes are heterogeneous, scattered in various modules and tangled with other SPL assets). 
%The modularity of transformations in CK and of \textit{build} process respond only to product line's variability management supported in \hp{}.
On the other hand, in \hpl{} the changes to address new assets are classified as homogeneous and localized since they only refer the definition of SPL asset meta data informations in a single \texttt{MetaData.hs} module. Thus, in \hpl{} there is more possibility of automation from a process of inference into the SPL asset modules implemented according to design rules previously defined.
We had a initial greater effort in the development of \hpl{} with the participation of a Haskell expert creating an infrastructure that contributes to a reduced effort to address the changes related to new assets when compared to the effort for the same purpose observed in \hp.

Furthermore, in \hpl{} some degree of modularity was obtained by the mapping of \hpl's product configurations to metaprogramming transformations where the \hpl's variability related to the assets' configuration is handled dynamically and automatically in the generation of a \hpl{} variant. This is allowed by the transformational process of generating products in \hpl{} with the support of the metaprogramming operations that meet the needs of managing the variability of assets in the artifacts (open data types and open functions) of the base module of \hpl.

%In addition, in \hpl{} the features represented by assets have their implementation entirely confined to specific modules, like the four modules that define respectively the algebraic data types, transformations, parser and output format of the asset.

%In addition to these results, we make some considerations on important issues like type safety and...

%Finally, regarding type safety, the generated instances are well-formed. A formal reasoning for type safety of the generation is beyond of the scope of this work. Instead, in the following, we present an analytical argument on type safety of the generated variants of \hpl. The \textit{build} performed by the kernel operates on a base product, which itself is well-formed. The metaprogramming transformations performed by the kernel are any of the following: add field, add constructor, add sentences in function, add import clause. Each of these takes a well formed program as input and generates a well-formed program as output...

\subsection{Threats to Validity} \label{sec:threats}

The threats to validity of our study can be addressed as follows:

\begin{itemize}

\item \emph{Construct validity} concerns establishing correct operational measures for the concepts being studied. The main constructs in our study are the concepts of ``configurability" and ``flexibility". Regarding the first, we defined metrics considering the underlying variability and effort of adding a configuration. As for the second construct, we defined metrics considering the evolution effort to address a new asset as well as key internal characteristic such as modularity.

\item \emph{Internal validity} concerns establishing a causal relationship, whereby certain conditions are shown to lead to other conditions. The design decisions behind the architecture definition trace to solving variability issues demanded by enhanced configurability support. The proposed reactive process was conceived to address the flexibility issue.

\item \emph{External validity} concerns establishing the domain to which a study's findings can be generalized. Although our study focuses on a single tool,  we believe that its design and supporting reactive process could be used to improve configurability and flexibility in other SPL product derivation tools.

\item \emph{Reliability} concerns demonstrating that the operations of a study can be repeated with the same results. Given the design, implementation, and reactive process descriptions, we expect that replications of our study should offer results similar to ours.

\end{itemize}



